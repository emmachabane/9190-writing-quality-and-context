This folder contains files with the stories needed for experiments analyzing the relation between context and writing quality.

## Retouched Sixfold data
TL;DR: you will probably want to use the versions of the stories presented in `sixfold/ascii_texts` and catalogued in `filtered_overview.csv` for any experiments. In what follows I give more detail on how these (and the other retouched files) were created. There are other miscellanous things I have coded, but if your workflow differs from mine they may not be useful.

There are three modified things here: `filtered_overview.csv`, `sixfold/retouched_texts`, and `sixfold/ascii_texts`. These are, respectively, modifications of `overview.csv` and the sixfold `texts/` .txt files that appear on the server somerset (not mirrored here to avoid confusion). There is also a new file, `sixfold_competition_types.csv`, which classifies each of the competitionIds as either belonging to the fiction or poetry contest. I will explain what was done to create each of these.

### Texts
The .txt files in `sixfold/ascii_texts` represent all 3467 texts from the initial scraped Sixfold data deemed usable. They have been manually retouched so that at least the first few paragraphs are usable (often more than that) and use the ASCII charset. There is also an intermediate level of processing in `sixfold/retouched_texts`.

The files in `texts/` were created by Andrei by running a PDF-to-txt conversion tool on the submissions' PDFs on Sixfold. There are two main obstacles to using these .txt files directly: first, some number of the submissions are actually to poetry contests; and second, there are many formatting issues with the raw text files.

To address the contest submission type issue, I used the `overview.csv` file to collect all possible `competitionId` values (the script is the tersely named `read_overview_print_competitionId.py`). Then, I went on Sixfold and manually input each competition ID as a URL argument to access the competition page for each of the competitions, and noted whether the ID was assigned to a poetry contest or a short fiction contest. These values were manually recorded in `data/sixfold/sixfold_competition_types.csv`. We can then use this file when iterating over `overview.csv` to only accept those stories that are indeed fiction submissions.

To address the formatting issues, I first tried writing a script that would correct the major formatting issues, then run some heuristics to pick out the first sentence. The logic was that this approach would be imperfect, but that there would be an acceptable number of errors. After many revisions it was realized that this wouldn't lead to an acceptable number of defects: there was simply too much diversity in how the usable stories are formatted by the author to begin with, too many stories mistranscribed in unique ways (e.g. with particular whitespace patterns, particular mistranscribed characters due to the author's chosen font), and too many flatly unusable stories (i.e. corrupted encoding, blank files, stories taken down by uploading a PDF saying "story removed"). It was therefore determined necessary to manually retouch all ~3.5k text files.

I used the script `copy_short_stories.py` to copy all short stories (filtered on competition types and presence of a file) into the directory `retouched_texts`. I used vim to edit the stories, which was frankly a godsend. I don't quite remember the exact time breakdown, but looking over my records and commit timestamps to substantiate my memory, it seems I made edits at a rate of about 190 stories per hour. My rate of progress was a little variable — with the first few days of edits I only tried to fix the first couple pages of text; as time went on I got more fastidious, fixing multiple pages; but I later realized my progress was stalling and internalized the discipline just to edit the first couple pages. The upshot is that at the very least the first sentence will be good, and the first few paragraphs will be good, but there are no guarantees beyond that.

Finally, I ran `clean_whitespace.py` to again copy these manually retouched texts into a new folder, `ascii_texts`, and also to remove extraneous whitespace characters while doing so (zero-width and non-breaking spaces, tabs, carriage returns, and multiple space runs, all of which the PDF converter seems to have inserted). Lastly I used [konwert](https://github.com/taw/konwert/tree/master/konwert-1.8) to convert the files to ASCII encoding. The command I used (courtesy of Andrei) is `konwert -O utf8-ascii *`. Thus completes the conversion of the files.

For the reasons mentioned, if you want to run an experiment that requires analyzing more of the text than the first sentence/first n sentences, then I would suggest again going through the retouched texts and further editing them as necessary, then processing them the way described here to recreate the ascii texts. This is one of the reasons I retain both retouched texts and the ascii texts in the Git repo, not just in version control.

There are a couple miscellanous topics that relate to this.

First, in the process of manually retouching, many content-level edits (not formatting edits) were necessary to expose the first sentence of the text in a consistent way. For example, many stories open with an epigraph — this isn't the opening sentence, but it's also clearly more related to the content of the story than just a page number or title or word count, which can be uncontroversially deleted. I therefore had to make some editorial decisions, many of which are of different types, and some of which were invoked with some contextual discretion. A choice clearly had to be made, but not without some controversy. I therefore recorded all edits I made (with the exception of certain very routine ones) in a markdown file, `data/sixfold/retouched_editorial_decisions.md`. Read that file to see more on what was done, which stories were affected, my reasoning for general types of edits, and my reasoning for particular exceptions.

Second, in manually editing the stories (and therefore skimming at least some of each, as this is necessary to make sure you really aren't deleting content, and you really are deleting non-content) I realized that there are actually a pretty substantial number of resubmissions. Andrei and Ignacio have said that these resubmissions are of particular note as they can provide a baseline to see whether e.g. there has been rating inflation over time. I wrote a script `find_duplicate_stories.py` which tries to find these resubmissions. You can read the script to see more, but the script considers two stories (by the same author) to be versions of each other if they have exactly identical titles, or if the stories have sufficiently similar content as measured by Levenshtein distance ratio. The ratio above which they are considered similar is a tunable parameter; from a few quick tests on known duplicates and known non-duplicates I have currently set it to 0.5, but more experimentation would be necessary to determine the best ratio.

Finally, the last script I wrote is for assigning story rankings to each story. The one obstacle here is that we don't have the number of stories submitted for each contest, so we can't just iterate through `filtered_overview.csv` and assign the top 25%/bottom 25%/middle labels on the first pass. The script passes through and collectes the attested final-ranks (the column called `final`) for each competition, and then treats the highest attested final-rank value as the number of submitted stories. This isn't strictly true, as there could be additional lower-ranked stories which didn't make it into our dataset for whatever reason — but it's as good as we can do.

There's now two things we can do with this information. The first is that we can take the top/bot 25% of stories from our _attested_ rankings and label them as such. The second is that we can use our (estimation of) the number of submitted stories to find the correct place cutoffs for top/bot 25% and label all stories falling into the relevant ranges based on their final ranks. These approaches are not identical because we do not have all submitted stories. The former approach will yield a dataset with an equal number of top/bot labeled stories, but it could potentially be very inaccurate — stories ranked fairly far down might qualify as "top 25%" because a lot of the stories above them are not attested. I chose to do it this former way because having a balanced dataset seemed more important to me than having exact rankings, but this choice could be examined more rigorously. It ultimately comes down to whether you want a dataset with good balance between top/bottom 25% story representation, or a dataset with ground-truth ranking labels. I wrote the script `collate_story_rankings.py` to generate these rankings and record them in two somewhat unfortunately named files, `story_ranks.csv` and `story_places.csv`. The `story_ranks.csv` file simply is a further filtered version of `filtered_overview.csv` only recording the columns relevant to calculating rankings. That file is in turn used to calculate the actual labels, which are then written down (alongside the extracted first sentence) in `story_places.csv`. That csv in turn is used to create a PyTorch dataloader in `models/sixfold_dataset.py`, but to be frank I wouldn't rely on any of these files since things have changed substantially since I last worked with them. The script should be a good scaffold for your own dataset creation, however.
